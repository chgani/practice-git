{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a3905bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cce3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"DataframeExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5bfa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Ganesh:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataframeExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2c86289fbe0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "393de259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.\\\n",
    "option('header','true').option('inferSchema',True).\\\n",
    "csv('C:/Users/ganes/OneDrive/Documents/pyspark/sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e325024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7b6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark=spark.read.csv('C:/Users/ganes/OneDrive/Documents/pyspark/sample.csv' \\\n",
    "                        ,header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cf911b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb89ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|    Krish| 31|        10|\n",
      "|Sudhanshu| 30|         8|\n",
      "|    Sunny| 29|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "337f2573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12350960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba6783ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', Age=31, Experience=10),\n",
       " Row(Name='Sudhanshu', Age=30, Experience=8),\n",
       " Row(Name='Sunny', Age=29, Experience=4)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "498ca319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7151b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|    Krish| 31|\n",
      "|Sudhanshu| 30|\n",
      "|    Sunny| 29|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7997d748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76a519f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8290856",
   "metadata": {},
   "source": [
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b32615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+-----------------+\n",
      "|summary| Name| Age|       Experience|\n",
      "+-------+-----+----+-----------------+\n",
      "|  count|    3|   3|                3|\n",
      "|   mean| null|30.0|7.333333333333333|\n",
      "| stddev| null| 1.0|3.055050463303893|\n",
      "|    min|Krish|  29|                4|\n",
      "|    max|Sunny|  31|               10|\n",
      "+-------+-----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa872da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Adding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bef926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=df_pyspark.withColumn('Experience after 2 years',df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f06fa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------------------------+\n",
      "|     Name|Age|Experience|Experience after 2 years|\n",
      "+---------+---+----------+------------------------+\n",
      "|    Krish| 31|        10|                      12|\n",
      "|Sudhanshu| 30|         8|                      10|\n",
      "|    Sunny| 29|         4|                       6|\n",
      "+---------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a39bd10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|    Krish| 31|        10|\n",
      "|Sudhanshu| 30|         8|\n",
      "|    Sunny| 29|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Drop the column\n",
    "\n",
    "sc1=sc.drop('Experience after 2 years')\n",
    "sc1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "342657c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "| New Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|    Krish| 31|        10|\n",
      "|Sudhanshu| 30|         8|\n",
      "|    Sunny| 29|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##reanming column\n",
    "\n",
    "sc1.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "133981fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.read.format(\"csv\").option(\"header\",True)\\\n",
    ".option(\"inferSchema\",True).option(\"path\",\"sample1.csv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fd428cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "| Ganesh|  28|         3| 50000|\n",
      "|shankar|  31|         2| 30000|\n",
      "|   Ramu|  26|         8| 50000|\n",
      "|   Babu|  25|         9| 35000|\n",
      "| Dinesh|  24|         1| 20000|\n",
      "| Sekhar|  28|        10| 10000|\n",
      "|  Pandu|  38|         6| 60000|\n",
      "|   Nagu|  40|         9| 70000|\n",
      "|    Sai|  41|         8| 36000|\n",
      "| Chandi|null|      null| 40000|\n",
      "|   null|null|         2|  null|\n",
      "|   null|  34|         3| 38000|\n",
      "|   null|  16|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "640e89cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "| Ganesh| 28|         3| 50000|\n",
      "|shankar| 31|         2| 30000|\n",
      "|   Ramu| 26|         8| 50000|\n",
      "|   Babu| 25|         9| 35000|\n",
      "| Dinesh| 24|         1| 20000|\n",
      "| Sekhar| 28|        10| 10000|\n",
      "|  Pandu| 38|         6| 60000|\n",
      "|   Nagu| 40|         9| 70000|\n",
      "|    Sai| 41|         8| 36000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "681e60f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "| Ganesh| 28|         3| 50000|\n",
      "|shankar| 31|         2| 30000|\n",
      "|   Ramu| 26|         8| 50000|\n",
      "|   Babu| 25|         9| 35000|\n",
      "| Dinesh| 24|         1| 20000|\n",
      "| Sekhar| 28|        10| 10000|\n",
      "|  Pandu| 38|         6| 60000|\n",
      "|   Nagu| 40|         9| 70000|\n",
      "|    Sai| 41|         8| 36000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56977338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "| Ganesh|  28|         3| 50000|\n",
      "|shankar|  31|         2| 30000|\n",
      "|   Ramu|  26|         8| 50000|\n",
      "|   Babu|  25|         9| 35000|\n",
      "| Dinesh|  24|         1| 20000|\n",
      "| Sekhar|  28|        10| 10000|\n",
      "|  Pandu|  38|         6| 60000|\n",
      "|   Nagu|  40|         9| 70000|\n",
      "|    Sai|  41|         8| 36000|\n",
      "| Chandi|null|      null| 40000|\n",
      "|   null|null|         2|  null|\n",
      "|   null|  34|         3| 38000|\n",
      "|   null|  16|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3668dc1",
   "metadata": {},
   "source": [
    "Signature:\n",
    "df1.na.drop(\n",
    "    how: str = 'any',\n",
    "    thresh: Optional[int] = None,\n",
    "    subset: Union[str, Tuple[str, ...], List[str], NoneType] = None,\n",
    ") -> pyspark.sql.dataframe.DataFrame\n",
    "Docstring:\n",
    "Returns a new :class:`DataFrame` omitting rows with null values.\n",
    ":func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
    "\n",
    ".. versionadded:: 1.3.1\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "how : str, optional\n",
    "    'any' or 'all'.\n",
    "    If 'any', drop a row if it contains any nulls.\n",
    "    If 'all', drop a row only if all its values are null.\n",
    "thresh: int, optional\n",
    "    default None\n",
    "    If specified, drop rows that have less than `thresh` non-null values.\n",
    "    This overwrites the `how` parameter.\n",
    "subset : str, tuple or list, optional\n",
    "    optional list of column names to consider.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> df4.na.drop().show()\n",
    "+---+------+-----+\n",
    "|age|height| name|\n",
    "+---+------+-----+\n",
    "| 10|    80|Alice|\n",
    "+---+------+-----+\n",
    "File:      c:\\users\\ganes\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\n",
    "Type:      method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47fde39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "| Ganesh|  28|         3| 50000|\n",
      "|shankar|  31|         2| 30000|\n",
      "|   Ramu|  26|         8| 50000|\n",
      "|   Babu|  25|         9| 35000|\n",
      "| Dinesh|  24|         1| 20000|\n",
      "| Sekhar|  28|        10| 10000|\n",
      "|  Pandu|  38|         6| 60000|\n",
      "|   Nagu|  40|         9| 70000|\n",
      "|    Sai|  41|         8| 36000|\n",
      "| Chandi|null|      null| 40000|\n",
      "|   null|  34|         3| 38000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop(how='any',thresh=2).show()\\\n",
    "#removes rows with lessthan 2 non null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58d0ea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "| Ganesh|  28|         3| 50000|\n",
      "|shankar|  31|         2| 30000|\n",
      "|   Ramu|  26|         8| 50000|\n",
      "|   Babu|  25|         9| 35000|\n",
      "| Dinesh|  24|         1| 20000|\n",
      "| Sekhar|  28|        10| 10000|\n",
      "|  Pandu|  38|         6| 60000|\n",
      "|   Nagu|  40|         9| 70000|\n",
      "|    Sai|  41|         8| 36000|\n",
      "|   null|null|         2|  null|\n",
      "|   null|  34|         3| 38000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop(how='any',subset=['Experience']).show()\n",
    "#if experience column having nulls it will remove the row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d92f4",
   "metadata": {},
   "source": [
    "Signature:\n",
    "df1.na.fill(\n",
    "    value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]],\n",
    "    subset: Optional[List[str]] = None,\n",
    ") -> pyspark.sql.dataframe.DataFrame\n",
    "Docstring:\n",
    "Replace null values, alias for ``na.fill()``.\n",
    ":func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
    "\n",
    ".. versionadded:: 1.3.1\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "value : int, float, string, bool or dict\n",
    "    Value to replace null values with.\n",
    "    If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
    "    from column name (string) to replacement value. The replacement value must be\n",
    "    an int, float, boolean, or string.\n",
    "subset : str, tuple or list, optional\n",
    "    optional list of column names to consider.\n",
    "    Columns specified in subset that do not have matching data type are ignored.\n",
    "    For example, if `value` is a string, and subset contains a non-string column,\n",
    "    then the non-string column is simply ignored.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> df4.na.fill(50).show()\n",
    "+---+------+-----+\n",
    "|age|height| name|\n",
    "+---+------+-----+\n",
    "| 10|    80|Alice|\n",
    "|  5|    50|  Bob|\n",
    "| 50|    50|  Tom|\n",
    "| 50|    50| null|\n",
    "+---+------+-----+\n",
    "\n",
    ">>> df5.na.fill(False).show()\n",
    "+----+-------+-----+\n",
    "| age|   name|  spy|\n",
    "+----+-------+-----+\n",
    "|  10|  Alice|false|\n",
    "|   5|    Bob|false|\n",
    "|null|Mallory| true|\n",
    "+----+-------+-----+\n",
    "\n",
    ">>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
    "+---+------+-------+\n",
    "|age|height|   name|\n",
    "+---+------+-------+\n",
    "| 10|    80|  Alice|\n",
    "|  5|  null|    Bob|\n",
    "| 50|  null|    Tom|\n",
    "| 50|  null|unknown|\n",
    "+---+------+-------+\n",
    "File:      c:\\users\\ganes\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\n",
    "Type:      method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe237dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e6afc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| Age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|       Ganesh|  28|         3| 50000|\n",
      "|      shankar|  31|         2| 30000|\n",
      "|         Ramu|  26|         8| 50000|\n",
      "|         Babu|  25|         9| 35000|\n",
      "|       Dinesh|  24|         1| 20000|\n",
      "|       Sekhar|  28|        10| 10000|\n",
      "|        Pandu|  38|         6| 60000|\n",
      "|         Nagu|  40|         9| 70000|\n",
      "|          Sai|  41|         8| 36000|\n",
      "|       Chandi|null|      null| 40000|\n",
      "|missing value|null|         2|  null|\n",
      "|missing value|  34|         3| 38000|\n",
      "|missing value|  16|      null|  null|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.fill(value='missing value').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "073c4c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "| Ganesh| 28|         3| 50000|\n",
      "|shankar| 31|         2| 30000|\n",
      "|   Ramu| 26|         8| 50000|\n",
      "|   Babu| 25|         9| 35000|\n",
      "| Dinesh| 24|         1| 20000|\n",
      "| Sekhar| 28|        10| 10000|\n",
      "|  Pandu| 38|         6| 60000|\n",
      "|   Nagu| 40|         9| 70000|\n",
      "|    Sai| 41|         8| 36000|\n",
      "| Chandi|  0|         0| 40000|\n",
      "|   null|  0|         2|  null|\n",
      "|   null| 34|         3| 38000|\n",
      "|   null| 16|         0|  null|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.fill(0,['Age','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d9cf6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "| Ganesh|  28|         3| 50000|\n",
      "|shankar|  31|         2| 30000|\n",
      "|   Ramu|  26|         8| 50000|\n",
      "|   Babu|  25|         9| 35000|\n",
      "| Dinesh|  24|         1| 20000|\n",
      "| Sekhar|  28|        10| 10000|\n",
      "|  Pandu|  38|         6| 60000|\n",
      "|   Nagu|  40|         9| 70000|\n",
      "|    Sai|  41|         8| 36000|\n",
      "| Chandi|null|      null| 40000|\n",
      "|   null|null|         2|  null|\n",
      "|   null|  34|         3| 38000|\n",
      "|   null|  16|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4b58805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer =Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c)\\\n",
    "                for c in ['Age','Experience','Salary']]\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07c9bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2=imputer.fit(df1).transform(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbdebe",
   "metadata": {},
   "source": [
    "##filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc0c16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.na.drop(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5608a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop multiple columns\n",
    "df4=df3.drop(*['Age','Experience','Salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec352618",
   "metadata": {},
   "outputs": [],
   "source": [
    "##rename multiple columns\n",
    "df5=df4.withColumnRenamed('Age_imputed','Age').\\\n",
    "withColumnRenamed('Experience_imputed','Experience').\\\n",
    "withColumnRenamed('Salary_imputed','Salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab5518b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "| Ganesh| 28|         3| 50000|\n",
      "|shankar| 31|         2| 30000|\n",
      "|   Ramu| 26|         8| 50000|\n",
      "|   Babu| 25|         9| 35000|\n",
      "| Dinesh| 24|         1| 20000|\n",
      "| Sekhar| 28|        10| 10000|\n",
      "|  Pandu| 38|         6| 60000|\n",
      "|   Nagu| 40|         9| 70000|\n",
      "|    Sai| 41|         8| 36000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cab11636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Pandu| 38|         6| 60000|\n",
      "| Nagu| 40|         9| 70000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.filter(\"Salary>50000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7c9210d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Pandu| 38|\n",
      "| Nagu| 40|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.filter(\"Salary>50000\").select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "659ca6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Ganesh| 28|\n",
      "|  Ramu| 26|\n",
      "|  Babu| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.filter(\"Salary>30000 and age <30\").select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aeb2ef5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdepartment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData science\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIOT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData science\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBig Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIOT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData Science\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBig Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIOT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData science\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:3035\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   3005\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3006\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[0;32m   3007\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3032\u001b[0m \n\u001b[0;32m   3033\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m-> 3035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[1;31mTypeError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "df5.withColumn(\"department\",(\"Data science\",\"IOT\",\"Data science\",\"Big Data\",\"IOT\"\\\n",
    "                                 \"Data Science\",\"Big Data\",\"IOT\",\"Data science\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7287aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=spark.read.format(\"csv\").option(\"header\",True).\\\n",
    "option(\"inferSchema\",True).\\\n",
    "option(\"path\",\"sample2.csv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "737b3d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   Name|  Department|Salary|\n",
      "+-------+------------+------+\n",
      "| Ganesh|Data science| 50000|\n",
      "|shankar|    Big Data| 30000|\n",
      "|   Ramu|         IOT| 50000|\n",
      "|   Babu|    Big Data| 35000|\n",
      "| Dinesh|         IOT| 20000|\n",
      "| Sekhar|Data science| 10000|\n",
      "|  Pandu|         IOT| 60000|\n",
      "|   Nagu|Data science| 70000|\n",
      "|    Sai|    Big Data| 36000|\n",
      "| Chandi|Data science| 40000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0579f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.createOrReplaceTempView(\"employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3dbdb71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|  Department|count(2)|\n",
      "+------------+--------+\n",
      "|         IOT|       2|\n",
      "|Data science|       3|\n",
      "|    Big Data|       3|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select Department,count(2) from employee where Salary>20000 \n",
    "group by Department\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f25b38f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  Department|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     130000|\n",
      "|Data science|     170000|\n",
      "|    Big Data|     101000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.groupBy('Department').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "12e6321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|  Department|       avg(Salary)|\n",
      "+------------+------------------+\n",
      "|         IOT|43333.333333333336|\n",
      "|Data science|           42500.0|\n",
      "|    Big Data|33666.666666666664|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.groupBy('Department').mean('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e2db9900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|     401000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.agg({'salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d375949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "| Ganesh| 28|         3| 50000|\n",
      "|shankar| 31|         2| 30000|\n",
      "|   Ramu| 26|         8| 50000|\n",
      "|   Babu| 25|         9| 35000|\n",
      "| Dinesh| 24|         1| 20000|\n",
      "| Sekhar| 28|        10| 10000|\n",
      "|  Pandu| 38|         6| 60000|\n",
      "|   Nagu| 40|         9| 70000|\n",
      "|    Sai| 41|         8| 36000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1e34e",
   "metadata": {},
   "source": [
    "#MLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1ba95295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d8742b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Age,Experience]----> new feature -------->independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c2156b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e0898688",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureAssembler=VectorAssembler(inputCols=[\"Age\",\"Experience\"],outputCol=\"independent feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e361cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=featureAssembler.transform(df5) #df5 will be training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b4ed6454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------------------+\n",
      "|   Name|Age|Experience|Salary|independent feature|\n",
      "+-------+---+----------+------+-------------------+\n",
      "| Ganesh| 28|         3| 50000|         [28.0,3.0]|\n",
      "|shankar| 31|         2| 30000|         [31.0,2.0]|\n",
      "|   Ramu| 26|         8| 50000|         [26.0,8.0]|\n",
      "|   Babu| 25|         9| 35000|         [25.0,9.0]|\n",
      "| Dinesh| 24|         1| 20000|         [24.0,1.0]|\n",
      "| Sekhar| 28|        10| 10000|        [28.0,10.0]|\n",
      "|  Pandu| 38|         6| 60000|         [38.0,6.0]|\n",
      "|   Nagu| 40|         9| 70000|         [40.0,9.0]|\n",
      "|    Sai| 41|         8| 36000|         [41.0,8.0]|\n",
      "+-------+---+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "baf2ae1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary', 'independent feature']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8ac53a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalised_data=output.select(\"independent feature\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c39f7269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|independent feature|Salary|\n",
      "+-------------------+------+\n",
      "|         [28.0,3.0]| 50000|\n",
      "|         [31.0,2.0]| 30000|\n",
      "|         [26.0,8.0]| 50000|\n",
      "|         [25.0,9.0]| 35000|\n",
      "|         [24.0,1.0]| 20000|\n",
      "|        [28.0,10.0]| 10000|\n",
      "|         [38.0,6.0]| 60000|\n",
      "|         [40.0,9.0]| 70000|\n",
      "|         [41.0,8.0]| 36000|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalised_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c7d96ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "babf44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##train test split\n",
    "train_data,test_data=finalised_data.randomSplit([0.65,0.35])\n",
    "regressor=LinearRegression(featuresCol='independent feature', labelCol='Salary')\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c8199ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([2600.424, -269.0127])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "921c3ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-36942.810818070495"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "104b51fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|independent feature|Salary|\n",
      "+-------------------+------+\n",
      "|         [24.0,1.0]| 20000|\n",
      "|         [26.0,8.0]| 50000|\n",
      "|         [28.0,3.0]| 50000|\n",
      "|        [28.0,10.0]| 10000|\n",
      "|         [31.0,2.0]| 30000|\n",
      "|         [38.0,6.0]| 60000|\n",
      "|         [40.0,9.0]| 70000|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e7c3d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|independent feature|Salary|\n",
      "+-------------------+------+\n",
      "|         [25.0,9.0]| 35000|\n",
      "|         [41.0,8.0]| 36000|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "192c6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results=regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1669592f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------------+\n",
      "|independent feature|Salary|        prediction|\n",
      "+-------------------+------+------------------+\n",
      "|         [25.0,9.0]| 35000|25646.676196598157|\n",
      "|         [41.0,8.0]| 36000| 67522.47368875484|\n",
      "+-------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "64946407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20437.89874607834, 540575506.8147621)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78765066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f0f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2beb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a451f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951a0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a7e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528a7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd847d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722f036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95ec87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6dba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9c388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fe621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01699e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
