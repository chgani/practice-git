import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader
import org.datasyslab.geosparksql.utils.{Adapter, GeoSparkSQLRegistrator}
import java.io._

object processData1 {
  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.ERROR)

    def getListOfFiles(dir: String): List[String] = {
      val file = new File(dir)
      file.listFiles //.filter(_.isFile)
        .filter(_.getName.endsWith("rds"))
        .map(_.getPath).toList
    }

    var files = getListOfFiles("src/main/resources")
val size=files.size
    // files.foreach(println)
    val spark = SparkSession
      .builder()
      .master("local")
      .appName("SparkAndHive")
      .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse2")
      .enableHiveSupport()
      .getOrCreate()

    GeoSparkSQLRegistrator.registerAll(spark.sqlContext)
    for (i <- files) {
      var table=spark.emptyDataFrame.createOrReplaceTempView("table")
      val spatialRDD = ShapefileReader.readToGeometryRDD(spark.sparkContext, i)

      val rawSpatialDf = Adapter.toDf(spatialRDD, spark).createOrReplaceTempView("rawSpatialDf")

      spark.sql()

     }
      spark.sql("select * from rawSpatialDf").show


  }
}

needs improvements